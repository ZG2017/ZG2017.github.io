---
layout: default
title: Ge (John) Zhang
permalink: /research/
---

## üìö Academic Research

### [**E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce**](https://arxiv.org/abs/2511.04087)
- **Authors**: **Ge Zhang**, RD Ajwani, T Zheng, H Gu, Y Hu, W Guo, M Coates, Y Zhang
- **Abstract**: E-CARE is a LLM-augmented recommendation framework, that enables cost-efficient way for bringing LLM's commonsense reasoning singal into downstream recommendation task, such as query-item relevance prediction.
- **Key Contributions**:
  - Proposed a LLM-enhanced scalable framework that improves performance of recommendation system through **commonsense reasoning** ability of LLMs
  - Introduced a **3-stage training pipeline** that removes reliance on costly SFT, large-scale manual annotations, and real-time LLM inference during serving
  - Constructed a **reasoning factor graph** to store commonsense reasonings generated by **Llama-3.1-8B-Instruct** from query-item interactions
  - Incorporated **LLM uncertainty estimation** to refine reasoning factor graph and enhance robustness against wrong LLM reasoning generations.
  - Trained lightweight adapters that map queries and items into the reasoning factor space, enabling downstream classification models to leverage commonsense knowledge without time-consuming real-time LLM inference.
  - Demonstrated up to **12.8% Micro-F1** improvement in search relevance tasks across two datasets.
- **Pipeline Diagram**:
  - Reasoning factor graph generation:
<img src="/resources/ECARE_pipeline_v2.png" alt="E-CARE Pipeline" style="width: 100%; height: 300px; object-fit: contain; display: block; margin: 4px 0;" />
  - Search relevance task:
<img src="/resources/research_pipeline.png" alt="Research Pipeline" style="width: 100%; height: 400px; object-fit: contain; display: block; margin: 4px 0;" />

### [**Path-of-Thoughts: Robust Relational Reasoning with LLMs**](https://arxiv.org/abs/2412.17963)
- **Authors**: **Ge Zhang**, MA Alomrani, H Gu, J Zhou, Y Hu, B Wang, Q Liu, M Coates, Y Zhang, J Hao
- **Abstract**: Path-of-Thoughts (PoT) decomposes relational reasoning into graph extraction, path identification, and reasoning, achieving up to 21.3% improvements on long-chain benchmarks with few LLM calls and improved robustness.
- **Status**: Newest version is under review by the TMLR journal
- **Key Contributions**:
  - Addressed LLM limitations in **multi-hop relational reasoning** tasks (e.g., kinship, spatial relations)
  - Developed a structured reasoning pipeline of graph extraction, reasoning path identification, and relation deduction to improve reasoning reliability
  - Improved accuracy by up to **21.3%** across both thinking LLMs (**GPT-5**, **GPT-o1-mini**, **Claude-3.7-Sonnet**, etc.) and non-thinking LLMs (**GPT-4o**, **Llama-3-70B**, etc.) on four benchmarks
  - Demonstrated robustness to noisy descriptions and LLM extraction errors, outperforming prior neuro-symbolic approaches
  - Generated reusable relational graphs supporting internal data generation for **SFT of in-house LLMs** and other research on [**complex reasoning with LLMs**](https://arxiv.org/abs/2409.12437)
- **Pipeline Diagram**:
  ![Path-of-Thoughts Pipeline](/resources/pot_pipeline.png){: style="width: 100%; height: 350px; object-fit: contain;" }

### [**Sparse Decomposition of Graph Neural Networks**](https://arxiv.org/abs/2410.19723)
- **Authors**: Y Hu, M Zeng, **Ge Zhang**, P Rumiantsev, L Ma, Y Zhang, M Coates
- **Abstract**: We introduce a sparse decomposition framework that factorizes GNN hidden representations into a small set of basis components with sparse coefficients, improving interpretability and efficiency while retaining predictive performance.
- **Key Contributions**:
  - Formulated sparse factorization for GNN representations with an efficient optimization objective.
  - Provided analysis showing regularization and interpretability benefits without sacrificing expressivity.
  - Empirically demonstrated competitive accuracy with reduced complexity and clearer explanations across standard graph benchmarks.

### [**Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data**](https://arxiv.org/abs/2409.12437)
- **Authors**: J Zhou, A Ghaddar, **Ge Zhang**, L Ma, Y Hu, S Pal, M Coates, B Wang
- **Abstract**: We generate graph-structured synthetic data that encodes compositional relations and reasoning chains, and use it to augment LLM training/evaluation, yielding consistent gains on multi-step logical reasoning without task-specific fine-tuning.
- **Key Contributions**:
  - Developed procedures to synthesize graph-grounded reasoning instances and prompts covering diverse logical patterns.
  - Data augmentation curriculum that injects compositional structure and longer reasoning chains.
  - Improved accuracy and robustness on multi-step reasoning benchmarks compared to strong baselines.

### [**Optimized Design of THz Microstrip Antenna Based on Dual-Surfaced Multiple Split-Ring Resonators**](https://ieeexplore.ieee.org/document/8072920)
- **Authors**: **Ge Zhang**, S Pu, XY Xu, C Tao, JY Dun
- **Venue**: 2017 IEEE International Symposium on Antennas and Propagation & USNC/URSI, 2017

### [**Design of 60-GHz Microstrip Antenna Array Composed Through Circular Contour Feeding Line**](https://ieeexplore.ieee.org/document/7522931)
- **Authors**: **Ge Zhang**, S Pu, X Xu, Y Liu, C Wang
- **Venue**: 2016 Asia-Pacific International Symposium on Electromagnetic Compatibility, 2016

---

[‚Üê Back to Home](/)
